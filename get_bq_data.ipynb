{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from BigQuery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path, os\n",
    "import time\n",
    "from google.cloud import bigquery,storage\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow for downloading files from BigQuery (via Google Cloud Storage)\n",
    "\n",
    "#### Setup parameters in preparation for transferring files from BigQuery to Cloud Storage, and then to download them locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n"
     ]
    }
   ],
   "source": [
    "bq_file_fo_path='./'\n",
    "project_id = 'mitx-edx-data'\n",
    "bucket_name = 'mitx-edx-data'\n",
    "\n",
    "local_dl_folder = bq_file_fo_path\n",
    "\n",
    "credentials = 'xxx.json'\n",
    "###### THIS LINE ABOVE IS SUPER-SENSITIVE AND TAKE GOOD CARE TO PROTECT THIS FILE.\n",
    "\n",
    "print(local_dl_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are a couple of utility functions that hopefully you don't have to deal with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gbq_to_bucket(project_id,dataset,table_name,bucket_name,fo_path,credentials):\n",
    "    client = bigquery.Client.from_service_account_json(credentials) #activate bigquery access\n",
    "    \n",
    "    json_tables = ['problem_analysis','course_axis']\n",
    "    \n",
    "    if table_name in json_tables:\n",
    "        file_name = dataset+'-'+table_name+'.json'\n",
    "    else:\n",
    "        file_name = dataset+'-'+table_name+'.csv'\n",
    "        \n",
    "    print(file_name)\n",
    "    destination_uri = 'gs://{}/{}/{}'.format(bucket_name, fo_path, file_name)\n",
    "    dataset_ref = client.dataset(dataset, project=project_id)\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "    job_config = bigquery.job.ExtractJobConfig()\n",
    "    print(job_config)\n",
    "    if table_name in json_tables:\n",
    "        job_config.destination_format = 'NEWLINE_DELIMITED_JSON'\n",
    "    #job_config.dry_run = True   # Doesn't really work!\n",
    "    \n",
    "    extract_job = client.extract_table(table_ref, destination_uri, job_config=job_config)\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "    print(extract_job)\n",
    "    print('Exported {}:{}.{} to {}'.format(project_id, dataset, table_name, destination_uri))\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def download_gs_files_to_local(project_id,bucket_name,fo_path,file_name,local_dl_folder,credentials):\n",
    "    \n",
    "    gs_file_path = '{}/{}'.format(fo_path,file_name)\n",
    "    local_file_path = '{}/{}'.format(local_dl_folder,file_name)\n",
    "    # Initialise a client\n",
    "    storage_client = storage.Client.from_service_account_json(credentials) #activate bigquery access\n",
    "    # Create a bucket object for our bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)  #oh no! Buckets name doesn't contain the folder!\n",
    "    # Create a blob object from the filepath\n",
    "    blob = bucket.blob(gs_file_path) #Include the google storage folder names here.\n",
    "    # Download the file to a destination\n",
    "    blob.download_to_filename(local_file_path) #This is the local file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the datasets and tables that you want to download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fo_path = 'MITx_residential_data' #this is in gcp\n",
    "fo_path = 'temp_data_transfer' #this is in gcp\n",
    "dataset_lists = ['MITx__8_02r_9__2019_Spring'\n",
    "]\n",
    "table_lists = [#'problem_analysis',\n",
    "    ##'problem_check',\n",
    "    'person_course',\n",
    "    ##'problem_grades'\n",
    "    #'person_course_day',\n",
    "    #'time_on_task',\n",
    "    #'course_axis',\n",
    "    #'show_answer_stats_by_user',\n",
    "    ##'stats_for_problems',\n",
    "    #'video_axis',\n",
    "    #'video_stats',\n",
    "    #'video_stats_day',\n",
    "    #'person_course_video_watched',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell to get stuff from BQ via GCS\n",
    "\n",
    "There are 2 main segments in this: exporting from BQ to GCS and from GCS to local disk.\n",
    "\n",
    "The following cell is good when you have a small number of tables to download. However, if you have a large number of files to download, the preferable methods is via gsutil.\n",
    "\n",
    "https://cloud.google.com/storage/docs/quickstart-gsutil\n",
    "\n",
    "'''\n",
    "To do so, once the tables are exported from BigQuery to Google Storage, use gsutil in command line to\n",
    "download them to the local folder. First, change your directory to where you want to be, and then type:\n",
    "gsutil -m cp gs://mitx-edx-data/moocs/* ./\n",
    "\n",
    "m -> is the batch processing command which goes much faster than working interactively\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MITx__8_02r_9__2019_Spring-person_course.csv\n",
      "<google.cloud.bigquery.job.ExtractJobConfig object at 0x10eac4a20>\n",
      "<google.cloud.bigquery.job.ExtractJob object at 0x10eac49e8>\n",
      "Exported mitx-edx-data:MITx__8_02r_9__2019_Spring.person_course to gs://mitx-edx-data/temp_data_transfer/MITx__8_02r_9__2019_Spring-person_course.csv\n"
     ]
    }
   ],
   "source": [
    "#Get data: GBQ --> Google Cloud Storage --> local disk\n",
    "for dataset in dataset_lists:\n",
    "    for table_name in table_lists:\n",
    "        try:\n",
    "            #The following line exports the table from BigQuery to Google Cloud Storage\n",
    "            file_name = extract_gbq_to_bucket(project_id,dataset,table_name,bucket_name,fo_path, credentials)\n",
    "            \n",
    "            #The following line downloads the file from Google Cloud Storage to local folder\n",
    "            download_gs_files_to_local(project_id,bucket_name,fo_path,file_name,local_dl_folder, credentials)\n",
    "            time.sleep(1) #the download job takes time\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print('Failed to download {} due to error {}'.format(table_name, e))\n",
    "        \n",
    "#wc -l [filenames*] lets you see the line numbers in each file, that makes it easy to check manually against\n",
    "#the table details on BQ to make sure that all the data got downloaded. For a large number of files this step\n",
    "#has to be automated. Still TO DO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of datasets, tables, etc.\n",
    "\n",
    "These are non essential, but useful commands..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_S989r__2019_IAP__2019_IAP\n",
      "2_S989r__2019_IAP__2019_IAP_logs\n",
      "MITProfessionalX__6_BDX__2015_T3_latest\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client.from_service_account_json(credentials) \n",
    "datasets = list(client.list_datasets())\n",
    "\n",
    "\n",
    "for dataset in datasets[:3]:  # API request(s)\n",
    "    print(dataset.dataset_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.table.TableListItem object at 0x10eadfe48> problem_analysis {'_properties': {'kind': 'bigquery#table', 'id': 'mitx-edx-data:2_S989r__2019_IAP__2019_IAP.problem_analysis', 'tableReference': {'projectId': 'mitx-edx-data', 'datasetId': '2_S989r__2019_IAP__2019_IAP', 'tableId': 'problem_analysis'}, 'type': 'TABLE', 'creationTime': '1550860788859'}}\n",
      "problem_analysis\n",
      "<google.cloud.bigquery.table.TableListItem object at 0x10eadfc18> problem_grades {'_properties': {'kind': 'bigquery#table', 'id': 'mitx-edx-data:2_S989r__2019_IAP__2019_IAP.problem_grades', 'tableReference': {'projectId': 'mitx-edx-data', 'datasetId': '2_S989r__2019_IAP__2019_IAP', 'tableId': 'problem_grades'}, 'type': 'TABLE', 'creationTime': '1550860756864'}}\n",
      "problem_grades\n",
      "<google.cloud.bigquery.table.TableListItem object at 0x10eb31eb8> studentmodule {'_properties': {'kind': 'bigquery#table', 'id': 'mitx-edx-data:2_S989r__2019_IAP__2019_IAP.studentmodule', 'tableReference': {'projectId': 'mitx-edx-data', 'datasetId': '2_S989r__2019_IAP__2019_IAP', 'tableId': 'studentmodule'}, 'type': 'TABLE', 'creationTime': '1550860740964'}}\n",
      "studentmodule\n",
      "<google.cloud.bigquery.table.TableListItem object at 0x10eb31b38> user_info_combo {'_properties': {'kind': 'bigquery#table', 'id': 'mitx-edx-data:2_S989r__2019_IAP__2019_IAP.user_info_combo', 'tableReference': {'projectId': 'mitx-edx-data', 'datasetId': '2_S989r__2019_IAP__2019_IAP', 'tableId': 'user_info_combo'}, 'type': 'TABLE', 'creationTime': '1550860772811'}}\n",
      "user_info_combo\n"
     ]
    }
   ],
   "source": [
    "dataset_ref = {\n",
    "    \"datasetId\": \"2_S989r__2019_IAP__2019_IAP\", \n",
    "    \"projectId\": \"mitx-edx-data\"\n",
    "  }\n",
    "\n",
    "tables = list(client.list_tables(bigquery.dataset.DatasetReference(dataset_ref['projectId'], \n",
    "                                                                   dataset_ref['datasetId'])))\n",
    "for table_ in tables[:5]:\n",
    "    print(table_, table_.table_id, table_.__dict__)\n",
    "    #or\n",
    "    print(table_._properties['tableReference']['tableId'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
